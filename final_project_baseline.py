# -*- coding: utf-8 -*-
"""Final project-baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15_U4EuVEGCSfOgRIMshjdvrt2qB_WnSU
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LSTM, Dropout, Attention, Bidirectional, Conv1D, Flatten, MultiHeadAttention
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.ensemble import VotingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from tensorflow.keras.callbacks import ReduceLROnPlateau
# Load dataset
data_path = "/content/Penicillin fermentation process data set.xlsx"
df = pd.read_excel(data_path, skiprows=1)  # Skip the first row if it contains column headers

# Hyperparameters
MODEL_TYPE = "Regression"  # Choose from 'Regression', 'Transformer', 'CNN', 'LSTM'
NUM_FEATURE_LAYERS = 1  # Number of CNN/Transformer/LSTM/Regression layers
NUM_DENSE_LAYERS = 2  # Number of Dense layers after feature extraction
MODEL_UNITS = 64
DENSE_UNITS = 32
OUTPUT_DENSE_UNITS = 32
DROPOUT_RATE = 0.2
LEARNING_RATE = 0.001
EPOCHS = 100
BATCH_SIZE = 16
FILTERS = 64  # For CNN model
KERNEL_SIZE = 3  # For CNN model
NUM_HEADS = 4  # For Transformer model
TEST_SIZE = 0.2
USE_LR_SCHEDULING = True

# Define input and output variables dynamically
columns = df.columns.tolist()
input_features = columns[:-4]
output_features = columns[-4:]

# Convert dataframe to numeric values
df = df.apply(pd.to_numeric, errors='coerce')

# Drop any rows with NaN values
df.dropna(inplace=True)

# Preprocessing
scaler_x = StandardScaler()
scaler_y = StandardScaler()
X = scaler_x.fit_transform(df[input_features])
y = scaler_y.fit_transform(df[output_features])

# Reshape X for LSTM and CNN (batch_size, time_steps, features)
X = X.reshape(X.shape[0], X.shape[1], 1)

# Pick 10 specific data points for demonstration before splitting (fixed indices within valid range)
demo_indices = np.array([5, 25, 50, 75, 100, 150, 200, 250, 300, 350])
demo_X = X[demo_indices]
demo_y = y[demo_indices]
X = np.delete(X, demo_indices, axis=0)
y = np.delete(y, demo_indices, axis=0)

# Splitting dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)

# Define input layer
input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))

# Model selection with added control for number of layers
if MODEL_TYPE == "LSTM":
    x = input_layer
    for _ in range(NUM_FEATURE_LAYERS):
        x = Bidirectional(LSTM(MODEL_UNITS, return_sequences=True))(x)
    x = Attention()([x, x])
    x = tf.keras.layers.GlobalAveragePooling1D()(x)  # Alternative: Flatten()

elif MODEL_TYPE == "CNN":
    x = input_layer
    for _ in range(NUM_FEATURE_LAYERS):
        x = Conv1D(FILTERS, KERNEL_SIZE, activation='relu', padding='same')(x)
    x = Flatten()(x)

elif MODEL_TYPE == "Transformer":
    x = input_layer
    for _ in range(NUM_FEATURE_LAYERS):
        x = MultiHeadAttention(num_heads=NUM_HEADS, key_dim=MODEL_UNITS)(x, x)
    x = Flatten()(x)

elif MODEL_TYPE == "Regression":
    x = input_layer
    for _ in range(NUM_FEATURE_LAYERS):
        x = Dense(MODEL_UNITS, activation='relu')(x)
    x = Flatten()(input_layer)

# Stacked Dense layers
for _ in range(NUM_DENSE_LAYERS):
    x = Dense(DENSE_UNITS, activation='relu')(x)
    x = Dropout(DROPOUT_RATE)(x)

# Define separate outputs with explicit names
outputs = []
for feature in output_features:
    safe_feature_name = feature.replace(" ", "_")
    out = Dense(OUTPUT_DENSE_UNITS, activation='relu', name=f"{safe_feature_name}_hidden")(x)
    out = Dropout(DROPOUT_RATE, name=f"{safe_feature_name}_dropout")(out)
    out = Dense(1, name=safe_feature_name)(out)
    outputs.append(out)

# Build model
model = Model(inputs=input_layer, outputs=outputs)
model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='mse')

# Display model summary to show the number of parameters
model.summary()

# Train model with less verbose output
if USE_LR_SCHEDULING:
    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5, verbose=1)
    callbacks = [lr_scheduler]
else:
    callbacks = []

# Train model with learning rate scheduler if enabled
history = model.fit(
    X_train, [y_train[:, i] for i in range(y_train.shape[1])],
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_split=0.2,
    verbose=2,
    callbacks=callbacks  # Include scheduler callback if enabled
)

# Plot loss curve
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title(f'Model Training Loss ({MODEL_TYPE})')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.savefig(f"loss_curve_{MODEL_TYPE}.png")
plt.show()

# Evaluate model using RMSE
predictions = model.predict(X_test)
rmse_scores = []
for i, feature in enumerate(output_features):
    safe_feature_name = feature.replace(" ", "_")
    mse = mean_squared_error(y_test[:, i], predictions[i].flatten())
    rmse = np.sqrt(mse)
    rmse_scores.append(rmse)
    print(f'RMSE for {safe_feature_name}: {rmse:.4f}')

# Predict on demonstration data
demo_predictions = model.predict(demo_X)

# Create and display results table
results_table = pd.DataFrame({
    'Data Point': demo_indices,
    'Experimental Value': demo_y[:, 0],
    'Predicted Value': np.array(demo_predictions[0]).flatten(),
    'Error': np.abs(demo_y[:, 0]-np.array(demo_predictions[0]).flatten())/np.array(demo_predictions[0]).flatten()})
print(results_table)
results_table.to_csv(f"10pts_table_{MODEL_TYPE}.csv", index=False)

# Ensemble Learning with Voting Regressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

base_models = [
    ('rf', RandomForestRegressor(n_estimators=50, random_state=42)),
    ('gb', GradientBoostingRegressor(n_estimators=50, learning_rate=0.1, random_state=42))
]

ensemble_models = []
ensemble_scores = []

# Train separate ensemble models for each output variable
train_preds = np.column_stack(model.predict(X_train))
test_preds = np.column_stack(model.predict(X_test))

for i, feature in enumerate(output_features):
    safe_feature_name = feature.replace(" ", "_")
    ensemble = VotingRegressor(estimators=base_models)
    ensemble.fit(train_preds[:, i].reshape(-1, 1), y_train[:, i])
    score = ensemble.score(test_preds[:, i].reshape(-1, 1), y_test[:, i])
    ensemble_models.append(ensemble)
    ensemble_scores.append(score)
    print(f'Ensemble Model Score for {safe_feature_name}: {score:.4f} (Higher is better, best = 1)')